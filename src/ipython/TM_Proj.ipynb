{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path='C:/Users/user/2017_English_final/2017_English_final/GOLD/Subtask_A/'\n",
    "path='/Users/munde/PycharmProjects/cs474/data'\n",
    "def extract_txt(files):\n",
    "    raw=[]\n",
    "    for f in files:\n",
    "        raw=raw+open(path+'%s'%f,'r').readlines()\n",
    "    lines=[a.strip() for a in raw]\n",
    "    return lines\n",
    "\n",
    "def create_table(filenames):\n",
    "    #files have duplicate rows\n",
    "    txt=extract_txt(filenames)#['twitter-2013dev-A.txt','twitter-2013train-A.txt','twitter-2015train-A.txt'])\n",
    "    table=pd.DataFrame()\n",
    "    ids=[]\n",
    "    tweets=[]\n",
    "    polarity=[]\n",
    "    for t in txt:\n",
    "        t=t.split('\\t')\n",
    "        #tweet id\n",
    "        t_id=t[0]\n",
    "        #tweet polarity\n",
    "        t_pol=t[1]\n",
    "        #tweet text\n",
    "        t_text=t[2]\n",
    "        ids.append(t_id)\n",
    "        tweets.append(t_text)\n",
    "        polarity.append(t_pol)\n",
    "    table['ID']=ids\n",
    "    table['TWEET']=tweets\n",
    "    table['POLARITY']=polarity\n",
    "    #drop duplicates\n",
    "    table.drop_duplicates('ID', inplace=True)\n",
    "    return table\n",
    "#s_class=sentiment class: 'neutral','positive','negative'\n",
    "def preprocess(table,s_class):\n",
    "    #get subset of table where polarity == to s_class\n",
    "    table=table.loc[table['POLARITY']==s_class]\n",
    "    tokens=[]\n",
    "    for index,values in table.iterrows():\n",
    "        tokens=tokens+extract_tokens(values['TWEET'])\n",
    "    return tokens\n",
    "def extract_tokens(txt):\n",
    "    txt=txt.lower()\n",
    "    st_words=nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    tweet_tok=nltk.TweetTokenizer()\n",
    "    tokens=tweet_tok.tokenize(txt)\n",
    "    #tokens= [x for x in tokens if not re.match('[' + string.punctuation + ']+', x)]\n",
    "    #remove stop words\n",
    "    tokens=[x for x in tokens if x not in st_words]\n",
    "    #attach pos tag extract only nouns, verbs, adjectves\n",
    "    tags= ['JJ','JJR','JJS','NN','NNS','NNP','NNPS','RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    tokens=[(token,tag) for token, tag in nltk.pos_tag(tokens) if tag in tags]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=create_table()\n",
    "positive_tokens=preprocess(t,'positive')\n",
    "negative_tokens=preprocess(t,'negative')\n",
    "neutral_tokens=preprocess(t,'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter tokens: leave only those that appear in one group..later use pmi or idf to filter \n",
    "def filter_tokens(tokens, tokens_to_remove):\n",
    "    uniq_t=set(tokens)-set(tokens_to_remove)\n",
    "    tokens=[t for t in tokens if t in uniq_t]\n",
    "    return tokens\n",
    "positive_tokens=filter_tokens(positive_tokens,negative_tokens+neutral_tokens)\n",
    "negative_tokens=filter_tokens(negative_tokens,positive_tokens+neutral_tokens)\n",
    "neutral_tokens=filter_tokens(neutral_tokens,negative_tokens+positive_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_synonyms(tokens):\n",
    "    synonyms=[]\n",
    "    for token, tag in tokens:\n",
    "        synonyms=synonyms+get_syns(token,tag)\n",
    "    #append synonyms\n",
    "    tokens=set(tokens+synonyms)\n",
    "    return tokens\n",
    "def add_antonyms(tokens, antonym_tokens):\n",
    "    antonyms=[]\n",
    "    for token, tag in tokens:\n",
    "        antonyms=antonyms+get_ant(token,tag)\n",
    "    #append antonyms\n",
    "    antonym_tokens=set(list(antonym_tokens)+antonyms)\n",
    "    return antonym_tokens\n",
    "def get_ant(token,tag):\n",
    "    synsets=[]\n",
    "    if tag.startswith('N'):\n",
    "        synsets=wn.synsets(token, 'n')\n",
    "    elif tag.startswith('J'):\n",
    "        synsets=wn.synsets(token, 'a')\n",
    "    elif tag.startswith('R'):\n",
    "        synsets=wn.synsets(token, 'r')\n",
    "    else:\n",
    "        synsets=wn.synsets(token, 'v')\n",
    "    antonyms=[]\n",
    "    for s in synsets:\n",
    "        lemmas=s.lemmas()\n",
    "        for l in lemmas:\n",
    "            antonyms= antonyms+[(w.name(),tag) for w in l.antonyms()]\n",
    "    return antonyms\n",
    "\n",
    "def get_syns(token,tag):\n",
    "    synsets=[]\n",
    "    if tag.startswith('N'):\n",
    "        synsets=wn.synsets(token, 'n')\n",
    "    elif tag.startswith('J'):\n",
    "        synsets=wn.synsets(token, 'a')\n",
    "    elif tag.startswith('R'):\n",
    "        synsets=wn.synsets(token, 'r')\n",
    "    else:\n",
    "        synsets=wn.synsets(token, 'v')\n",
    "    synonyms=[]\n",
    "    for s in synsets:\n",
    "        synonyms= synonyms+[(w.name(),tag) for w in s.lemmas()]\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding synonyms\n",
    "positive_tokens=add_synonyms(positive_tokens)\n",
    "negative_tokens=add_synonyms(negative_tokens)\n",
    "neutral_tokens=add_synonyms(neutral_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding antonyms\n",
    "negative_tokens=add_antonyms(positive_tokens,negative_tokens)\n",
    "positive_tokens=add_antonyms(negative_tokens,positive_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(neutral_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering\n",
    "positive_tokens=filter_tokens(positive_tokens,set(negative_tokens)|set(neutral_tokens))\n",
    "negative_tokens=filter_tokens(negative_tokens,set(positive_tokens)|neutral_tokens)\n",
    "neutral_tokens=filter_tokens(neutral_tokens,set(negative_tokens)|set(positive_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14782 9825 54240\n"
     ]
    }
   ],
   "source": [
    "#number of tokens in each class\n",
    "print len(positive_tokens), len(negative_tokens) , len(neutral_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove tags\n",
    "positive_tokens=[token for token, tag in positive_tokens]\n",
    "negative_tokens=[token for token, tag in negative_tokens]\n",
    "neutral_tokens=[token for token, tag in neutral_tokens]\n",
    "def calculate_polarity(tweet_tokens):\n",
    "    sentiment=0\n",
    "    for t in tweet_tokens:\n",
    "        if t in positive_tokens:\n",
    "            sentiment=sentiment+1\n",
    "        elif t in negative_tokens:\n",
    "            sentiment=sentiment-1\n",
    "    if sentiment>0:\n",
    "        return 'positive'\n",
    "    elif sentiment<0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing \n",
    "test_t=create_table(['twitter-2013test-A.txt','twitter-2013test-A.txt','twitter-2013test-A.txt'])\n",
    "sent_class=[]\n",
    "for index,values in test_t.iterrows():\n",
    "    tokens=[t for t,tag in extract_tokens(values['TWEET'])]\n",
    "    sentiment=calculate_polarity(tokens)\n",
    "    sent_class.append(sentiment)\n",
    "test_t['Class']=sent_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.422849083216\n"
     ]
    }
   ],
   "source": [
    "classified_correctly=0\n",
    "for index,values in test_t.iterrows():\n",
    "    \n",
    "    if values['POLARITY']==values['Class']:\n",
    "        classified_correctly+=1\n",
    "#accuracy\n",
    "print classified_correctly/len(test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
